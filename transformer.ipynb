{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNCSdlBMMQiaCHhEVqhwKsp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhanth301/transformers/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "gRrN1bD_iX2b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Input Embedding, Positional Encoding"
      ],
      "metadata": {
        "id": "BeTW1QmoiL64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_length, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.word_emb = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embed_dim)\n",
        "        self.pos_emb = nn.Embedding(\n",
        "            num_embeddings=max_length,\n",
        "            embedding_dim=embed_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, seq_len = x.size()\n",
        "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
        "        output1 = self.word_emb(x)\n",
        "        output2 = self.pos_emb(positions)\n",
        "        output =  output1 + output2\n",
        "        return output"
      ],
      "metadata": {
        "id": "bTLo8XWKiLJM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Encoder"
      ],
      "metadata": {
        "id": "Pgm3hN-wjlOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PZbGC5HvgYP4"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "        )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        attn_output, _ = self.attn(query, key, value)\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out_1 = self.layernorm_1(query + attn_output)\n",
        "        ffn_output = self.ffn(out_1)\n",
        "        ffn_output = self.dropout_2(ffn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + ffn_output)\n",
        "        return out_2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionEmbedding(src_vocab_size, embed_dim, max_length, device)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout) for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, output, output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "0iBwDE1Ulldf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Decoder"
      ],
      "metadata": {
        "id": "V8bjT3vBnevr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "        )\n",
        "        self.layernorm_1  = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2  = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_3  = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "        self.dropout_3 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask=tgt_mask)\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out_1 = self.layernorm_1(x + attn_output)\n",
        "        attn_output, _ = self.cross_attn(out_1, enc_output, enc_output, attn_mask=src_mask)\n",
        "        attn_output = self.dropout_2(attn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output)\n",
        "        ffn_output = self.ffn(out_2)\n",
        "        ffn_output = self.dropout_3(ffn_output)\n",
        "        out_3 = self.layernorm_3(out_2 + ffn_output)\n",
        "        return out_3"
      ],
      "metadata": {
        "id": "_YAknXPDnda3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, tgt_vocab_size, embed_dim, max_length, num_layers, num_aheads, ff_dim, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionEmbedding(tgt_vocab_size, embed_dim, max_length, device)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerDecoderBlock(embed_dim, num_heads, ff_dim, dropout) for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        output = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_output, src_mask, tgt_mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "4hqe11zKp22G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Transformer"
      ],
      "metadata": {
        "id": "Q5aQB5Eaq9af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.encoder = TransformerEncoder(src_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout, device)\n",
        "        self.decoder = TransformerDecoder(tgt_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout, device)\n",
        "        self.fc = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_seq_len = src.shape[1]\n",
        "        tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "        src_mask = torch.zeros((src_seq_len, src_seq_len), device=self.device).type(torch.bool)\n",
        "        tgt_mask = (torch.triu(torch.ones((tgt_seq_len, tgt_seq_len), device=self.device).type(torch.bool)) == 1).transpose(0,1)\n",
        "        tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        enc_output = self.encoder(src)\n",
        "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "DQlggyhtq6l7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "src_vocab_size = 1000\n",
        "tgt_vocab_size = 2000\n",
        "embed_dim = 200\n",
        "max_length = 100\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "ff_dim = 256\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dropout = 0.1\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim)\n",
        "src = torch.randint(high=2, size=(batch_size, max_length), dtype=torch.int64)\n",
        "tgt = torch.randint(high=2, size=(batch_size, max_length), dtype=torch.int64)\n",
        "prediction = model(src, tgt)\n",
        "prediction.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d09fVYhltVN4",
        "outputId": "48b44581-b63d-4e45-f7ae-a35ffd951eb3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 100, 2000])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}